# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pMUKdbO55QLQgIg4rskEZIN8EVT9LvrG
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer

# Load the dataset
df = pd.read_csv('/content/The_Case_of_the_Predictive_Crime_Solver.csv')

# Handle missing values if any
df.fillna(method='ffill', inplace=True)

# Convert 'DATE OCC' and 'TIME OCC' to datetime
df['DATE OCC'] = pd.to_datetime(df['DATE OCC'], errors='coerce')
df['TIME OCC'] = pd.to_datetime(df['TIME OCC'], format='%H%M', errors='coerce').dt.time

# Extract features from datetime
df['hour'] = df['TIME OCC'].apply(lambda x: x.hour if pd.notnull(x) else 0)
df['day'] = df['DATE OCC'].dt.day
df['month'] = df['DATE OCC'].dt.month
df['year'] = df['DATE OCC'].dt.year

# Ensure 'Vict Sex' is numeric
df['Vict Sex'] = df['Vict Sex'].map({'M': 1, 'F': 0})

# Convert 'Vict Descent' to numeric
df['Vict Descent'] = df['Vict Descent'].astype('category').cat.codes

# Combine the other features using CountVectorizer to handle text data
vectorizer = CountVectorizer()
other_features = df[['Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Desc', 'Weapon Used Cd', 'Status', 'Crm Cd 1']].astype(str).apply(' '.join, axis=1)
other_features_vectorized = vectorizer.fit_transform(other_features)

# Combine vectorized features with other numeric features
other_features_df = pd.DataFrame(other_features_vectorized.toarray(), columns=vectorizer.get_feature_names_out())
df = pd.concat([df, other_features_df], axis=1)

# Define target variable
df['outcome'] = df['Crm Cd Desc']

# Drop original columns that have been processed
columns_to_drop = ['Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Crm Cd',
                   'Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc',
                   'Status', 'Status Desc', 'Crm Cd 1', 'LOCATION', 'Cross Street', 'LAT', 'LON', 'Crm Cd Desc']
df = df.drop(columns=columns_to_drop)

# Define features and target variable
X = df.drop(columns=['outcome'])
y = df['outcome']

# Include extracted date components in features
selected_features = ['hour', 'day', 'month', 'year'] + list(other_features_df.columns)
X = X[selected_features]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Dimensionality Reduction
pca = PCA(n_components=5)  # Adjust the number of components as needed
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

# Model Building using SVM
svm = SVC()

# Hyperparameter Tuning using GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear']
}

grid_search = GridSearchCV(svm, param_grid, refit=True, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters found
print(f"Best Parameters: {grid_search.best_params_}")

# Predictions
y_pred = grid_search.predict(X_test)

# Evaluation
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))

# Future Crime Description (Example Function)
def describe_future_crime(features, model, scaler, pca):
    features = scaler.transform([features])
    features = pca.transform(features)
    prediction = model.predict(features)
    return f"Predicted Crime Outcome: {prediction[0]}"

# Example usage of the describe_future_crime function
# Note: Ensure the features passed match the preprocessed format
future_features = [12, 16, 7, 2024]  # Example date components
print(describe_future_crime(future_features, grid_search, scaler, pca))

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


data = pd.read_csv('/content/The_Case_of_the_Predictive_Crime_Solver.csv')

numerical_data = data.select_dtypes(include=['number'])

corr = numerical_data.corr()


plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Heatmap of Correlation Matrix (Numerical Values Only)')
plt.show()


numerical_data = data.select_dtypes(include=['number'])
feature1 = 'TIME OCC'
feature2 = 'LAT'
plt.figure(figsize=(10, 6))
sns.scatterplot(x=feature1, y=feature2, data=numerical_data)
plt.title(f'Scatter Plot of {feature1} vs {feature2}')
plt.xlabel(feature1)
plt.ylabel(feature2)
plt.show()


numerical_data = data.select_dtypes(include=['number'])

numerical_data.hist(bins=15, figsize=(15, 10))
plt.tight_layout()
plt.show()


grouped_data = data.groupby(['Vict Sex', 'Premis Desc']).size().unstack()
grouped_data.plot(kind='bar', stacked=False, figsize=(10, 6))
plt.title('Victim Sex by Premise Description')
plt.xlabel('Victim Sex')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Premise Description')
plt.show()

